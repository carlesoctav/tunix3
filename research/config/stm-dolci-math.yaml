# Selective Token Masking (STM) SFT Training Config - Math Dataset
# Based on: "Mitigating Forgetting in LLM Fine-Tuning via Low-Perplexity Token Learning"
# arXiv:2501.14315

exp_name: stm-dolci-math-50k
project: tunix-sft  # WandB project name

model_args:
  model_family: Gemma3
  model_name: gemma3-1b-it
  model_id: google/gemma-3-1b-it
  hf_tokenizer_path: google/gemma-3-1b-it
  mesh_axis_names: [fsdp, tp]
  mesh_shape: [4, 1]
  remat: BLOCK  # Enable gradient checkpointing to reduce memory usage
  rng_seed: 42
  lora_config:
    rank: 64
    alpha: 64.0
    module_path: ".*q_einsum|.*kv_einsum|.*gate_proj|.*down_proj|.*up_proj|.*attn_vec_einsum"

training_args:
  max_steps: 6545  # 5 epochs (1309 steps/epoch * 5)
  eval_every_n_steps: 1000
  gradient_accumulation_steps: 4  # Effective batch = 8 * 4 = 32
  checkpoint_root_directory: gs://carles-git-good/tunix-sft
  log_dir: /mnt/carles/logs
  flush_every_n_steps: 20
  checkpointing_options:
    save_interval_steps: 500
    max_to_keep: 4
  optimizer_config:
    opt_type: adam
    peak_value: 3e-5
    init_value: 0.0
    end_value: 0.0
    warmup_ratio: 0.1
    warmup_steps: null
    decay_steps: null
    b1: 0.9
    b2: 0.99
    weight_decay: 0
    max_grad_norm: 1.0
    schedule_type: warmup_cosine_decay_schedule

data_args:
  path: carlesoctav/dolci-50k-27b
  name: null
  split: train
  eval_split: null 
  split_ratio: 0.1  # 10% for eval, 90% for train
  max_seq_length: 4096
  num_train_examples: null
  num_eval_examples: 1000  # Limit eval to 1000 examples
  prompt_column: prompt
  answer_column: generated
  batch_size: 8  # Micro batch size per forward pass (effective = 8 * 4 = 32)
  shuffle: true
  shuffle_seed: 42
  epochs: 5  # Train for 5 epochs
  chat_template_path: ./gemma_think_new.jinja
  tokenizer_path: google/gemma-3-1b-it 

stm_config:
  enabled: true
  ppl_threshold: 2.5  # Mask tokens with perplexity > 2.5 (paper's optimal)

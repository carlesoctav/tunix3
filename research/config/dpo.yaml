exp_name: dpo-gemma3-1b-dpo-sample
project: tunix-dpo

model_args:
  model_family: Gemma3
  model_name: gemma3-1b-it
  model_id: google/gemma-3-1b-it
  hf_tokenizer_path: google/gemma-3-1b-it
  mesh_axis_names: [fsdp, tp]
  mesh_shape: [4, 1]
  remat: BLOCK  
  rng_seed: 42
  use_ref_model: true
  lora_config:
    rank: 128
    alpha: 256 
    module_path: ".*q_einsum|.*kv_einsum|.*gate_proj|.*down_proj|.*up_proj|.*attn_vec_einsum"

training_args:
  max_steps: 5000
  eval_every_n_steps: 500
  gradient_accumulation_steps: 4  
  checkpoint_root_directory: gs://carles-git-good/tunix-dpo
  log_dir: /mnt/carles/logs
  flush_every_n_steps: 20
  checkpointing_options:
    save_interval_steps: 1000
    max_to_keep: 4
  # DPO-specific configs
  algorithm: dpo  # "dpo" or "orpo"
  beta: 0.1  # KL penalty weight for DPO
  lambda_orpo: 0.1  # Weight for preference loss (ORPO only)
  label_smoothing: 0.0
  max_prompt_length: 512
  max_response_length: 1024
  optimizer_config:
    opt_type: adamw
    peak_value: 5e-7
    init_value: 0.0
    end_value: 0.0
    warmup_ratio: 0.1
    warmup_steps: null
    decay_steps: null
    b1: 0.9
    b2: 0.99
    weight_decay: 0.01
    max_grad_norm: 1.0
    schedule_type: warmup_cosine_decay_schedule

data_args:
  path: carlesoctav/dpo-sample
  name: null
  split: train
  eval_split: null 
  split_ratio: 0.05
  max_prompt_length: 512
  max_response_length: 1024
  num_train_examples: null
  num_eval_examples: null  
  prompt_column: prompt
  chosen_column: chosen
  rejected_column: rejected
  batch_size: 2
  shuffle: true
  shuffle_seed: 42
  tokenizer_path: google/gemma-3-1b-it
